{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Data-Driven Approach to Job Discrimination Law\n",
    "## CS481 Assignment 1\n",
    "This assignment will place you and your team in the posture of a newly formed partnership between SprawlMart's legal and engineering departments.  We have read of rising concerns over the potentially discriminatory behavior of certain algorithmic hiring tools and now you'll be expected to address these concerns from technical, legal, and business perspectives.  SprawlMart's CEO and General Counsel have become increasingly wary of potential probes from the EEOC into its latest adoption of algorithmic hiring practices.  Moreover, apart from potential liability from the feds, a whole host of state legislatures have passed or begun to consider laws regulating the use of automated evaluation algorithms demanding explainability and transparency among other measures. \n",
    "\n",
    "Your team of Stanford-trained lawyers and engineers have been tasked with the job of analyzing potential problems in the latest dataset of candidates applying for SprawlMart's coveted Entry-Level Business/Data Analyst Position. You will be tasked with analyzing the liability risks that the company faces and prototyping a system that shields the corporation from those risks while balancing other key corporate objectives.\n",
    "\n",
    "### Learning Goals:\n",
    "* Gain basic command over concepts in data science and machine learning as it relates to legal statutes. \n",
    "* Gain a robust understanding of where technical tools can be used as leverage to demonstrate liability under particular statutes - in this case job discrimination law. \n",
    "\n",
    "Note to self:  Was trying to say something along the lines of don't feel overwhelmed, and encourage folks to contribute and help others with what they're good at but couldn't quite phrase it right...\n",
    "\n",
    "An aside:  One of the most exciting characteristics of this class is its diversity - this cohort ranges from C.S. PhDs developing improvements in Generative Adversarial Networks to first-year Law Students well-versed in contract and tort law, to second-year English major undergrads writing prose.  We are well aware that teaching a law class in the medium of a jupyter notebook is foreign and new.  For those worried about what programming knowledge which might be required, we did not expect that you came into this class with a technical background - only that you desired to gain some of those basic skills.  If at any point, you struggle with figuring out how something in the code is working, what is being written, or just require greater clarity over what is happening from a technical viewpoint, please do not hesitate to ask for help.  For those of you who are well-versed in data science, programming, machine learning, we imagine that you came here not to learn basic skills you already know deeply but likely to learn and grapple with the interesting legal puzzles and questions that arise at the intersection of the systems you build and the laws of the land. In sum, we hope that this will be a rich experience, where every party not only gains valuable skills but also has the opportunity to demonstrate their own skillset and assist their teammates. If at any point you feel that is not the case please offer your feedback! \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment Overview\n",
    "#### There will be 3 milestones to this assignment. \n",
    "1. Studying a proprietary dataset of candidates for potential legal liability under Job Discrimination statutes.  \n",
    "\n",
    "2. Building a compliant hiring algorithm that meets compliance measures and corporate objectives.  \n",
    "\n",
    "3. Analyzing performance, and considering the legal implications of fairness-correcting measures. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Milestone 1:  Analyzing the Data for Legal Liability \n",
    "To start your team's analysis and task, you've been handed the latest dataset of 500 candidates being evaluated for the next cohort of entry-level business analysts.  Moreover, along with the datset HR's engineering team has also kindly shared with you three basic algorithms they've used in the past to evaluate candidates.  In practice, the inclusion of this automated system is planned for use in the initial evaluation phrase, prior to any in-person interviews.  Those scoring in the top 30% will move on to the next phase of evaluation by an actual recruiter. \n",
    "\n",
    "Recall that under the existing federal statutes - there exists three evidentiary frameworks to detect discriminatory intent: direct, indirect, and statistical. ([Brookings](https://www.brookings.edu/research/assessing-employer-intent-when-ai-hiring-tools-are-biased/)).  We will focus on statistical evidence although we encourage you to repeatedly consider other potential sources of liability.  As discussed in lecture, the EEOC has adopted a 4/5ths rule of thumb to quantify a \"substantially different rate of selection\" that could lend to liability risk under the Title VII apart from a demonstration of business necessity (NEEDS CITATION). The courts have generally upheld the rule.  \n",
    "\n",
    "The 4/5ths rule states that if the selection rate for a certain group (X) is less than 80 percent of that of the group with the highest selection rate (Y), there is adverse impact on that group (X). [Types of Employment Discrimination](https://www.justice.gov/crt/types-employment-discrimination) \n",
    "\n",
    "\n",
    "#### Learning goals for this Milestone\n",
    "##### Job Discrimination Law\n",
    "* Acquire a deep understanding of where liability under the EEOC's 4/5ths rule may apply in the context of job discrimination.\n",
    "\n",
    "##### A Basic Technical Toolset\n",
    "* A basic but fundamental and actionable understanding of how data is used to make predictions\n",
    "* Learn how legal standards of liability connect with the analysis of datasets.\n",
    "* Exposure to more modern data sources for job evaluation, corresponding evaluation algorithms and their legal risk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Setting up our Analysis Pipeline\n",
    "\n",
    "Let's first load in the dataset that was sent to us and analyze its contents.  \n",
    "\n",
    "For those unfamiliar: We are using the progamming language Python!  It is an interpreted, high-level, general-purpose programming language. it emphasizes code readability and excels for coding applications in data science and machine learning!  We will be using a number of packages in python (think of them as additional applications, programs), to help us perform some of the tasks we need to do.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbimporter\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import walmartDataset\n",
    "import librosa\n",
    "import IPython.display as ipd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Dataset:\n",
    "* This dataset holds 500 individuals, carrying 22 data points on each of them.  Each of these data points are encoded in distinct manners ranging from minutes for commute time, to continuous GPA values, to discrete numbers on a scale for different skill scores.  \n",
    "* Each of the features can be seen below - think of the following as querying for those features that you're about to see defined!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the dataset .... this might take a few seconds!\n",
      "500\n",
      "500\n",
      "500\n",
      "500\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "features = [\"Names\", \"GPA\", \"Avg Commute Time\", \"Cultural Fit\", \"Technical Aptitude\", \n",
    "           \"Soft Skills\", \"Employee Referral\", \"Years of Experience\", \"Leadership Capability\", \n",
    "           \"Race\", \"Sex\", \"Age\", \"Birth Origin\", \"Education\", \n",
    "           \"Undergraduate Degree\", \"Educational Prestige\", \"Sports\", \"Criminal Record\",\n",
    "           \"Arrest Records\", \"LinkedIn Score\", \"Responsible Social Media Use\", \"HireVue Score\"]\n",
    "numCandidates = 500\n",
    "dataset = walmartDataset.loadDataset(numCandidates, features)\n",
    "dataset.to_csv(\"dataset.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1: Warmup:  Familiarizing ourselves with the data (Estimated Time: 1 hr)\n",
    "Anyone who works with data knows that one of the most important, if not the most important, aspects of working with it is knowing your dataset very well - what features there are, how they are encoded, what imbalances exist, and more. \n",
    "\n",
    "1.  How are each of these features currently encoded in the dataset? Try defining the domain of each feature. (If it is not clear for one of the features - hypothesize and point it out in your response).\n",
    "\n",
    "2.  What features currently, are the most worrisome to you from the viewpoint of incurring liability - please refer specifically to the state and federal statutes we have discussed in class.\n",
    "\n",
    "3. Are there ways in which particular features were encoded, that seem problematic to you?  If so, how would you recommend re-encoding them?  (Think explainability standards/statutes).\n",
    "\n",
    "4.  Now, explain how those very same features would be critical to Walmart's HR team in its selection process.  Cast your response in terms of the business necessity defense we discussed. \n",
    "\n",
    "5.  Can you detect any imbalances with the data? If so, please specify them in detail. A data imbalance is where the distribution of values within a given field is uneven.  One obvious one here is gender! (1-2 sentences per scenario)\n",
    "\n",
    "6. Merging your team's legal thinking cap with your algorithmic experience - provide three scenarios in which the data imbalances you found could plausibly lead to algorithmic outputs which seem to violate the statutes we have discussed.  (Expected Length: Paragraph per scenario)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Analyzing the prototyped models\n",
    "\n",
    "### The Models\n",
    "\n",
    "Now that you've familiarized yourself with the dataset and gained an intuition over potential worries, \n",
    "lets test your concerns with some of the models the HR Engineering team has provided you with \n",
    "\n",
    "#### The 3 Models:\n",
    "\n",
    "* A Ranking Model\n",
    "* A Nearest Neighbor Model\n",
    "* A Basic Neural Network Model\n",
    "\n",
    "\n",
    "#### Lets start with the ranking model!\n",
    "* This is about as simple as it can get for trying to evaluate candidates.\n",
    "* Here's how it works\n",
    "    * Select features to rank candidates based on\n",
    "    * Equally weight each of the features\n",
    "    * Take the sum of those values ==> that's their score\n",
    "    * Rank the candidates based on their score\n",
    "    * Impose a percentile cutoff - everyone above the cutoff makes it past the algorithmic evaluation round! \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureCombinations = {\n",
    "    \"combo1\": [\"GPA\", \"Employee Referral\", \"Soft Skills\", \"Leadership\", \"Arrest Records\"],\n",
    "    \"combo2\": [\"Technical Aptitude\", \"Educational Prestige\", \"Hirevue Score\", \"Commute Time\"],\n",
    "    \"combo3\": [\"Hirevue\", \"Linkedin Score\", \"Cultural Fit\"]\n",
    "}\n",
    "cutoff = .30\n",
    "# ranks = runRankingModel(walmartDataset)\n",
    "# visualize(ranks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q2: Interrogating the ranking model (30mins - 45mins)\n",
    "* For each of the feature sets given detail sources of liability based on your analyzing the model's performance. Please carefully distinguish and detail the claim and justification, providing visualizations of what you have found using the tools we have provided to you.\n",
    "\n",
    "* Now, if you see the code above, we equally weighted each of the different features we used to calculate the candidate's score.  Now try re-weighting each of the featureCombinations in a manner that you believe would resolve these claims.  Were you successful?\n",
    "\n",
    "* Notice how our cutoff is right at the top 30th percentile.  What if we changed the cutoff score - how would the performance of the model shift?  Try increasing and decreasing the cutoff in margins of .05. Would we be able to reduce the risk of liability - how would this affect our other corporate objectives though?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Nearest Neighbor Model\n",
    "\n",
    "We have 4 previous model employees that we've labelled in terms of their performance - their names are Jane, Oscar, John, and Beatrice. Jane is an excellent worker, Oscar is a good worker, John, is a satisfactory worker, and Beatrice was a poor worker. Moreover, we have the same 22 data points for each of them as we do for each of candidates. \n",
    "\n",
    "* This is how the model works:\n",
    "    * Choose the data points we believe are most expressive of our 4 model employees' performance\n",
    "    * For each of the candidates:\n",
    "        * For each of the model employees\n",
    "            * Calculate the \"distance\" between the candidate and the particular model employee\n",
    "        * Pick out the label of model employee that has the least distance, is closest, to the candidate\n",
    "        * Assign that label to the candidate.\n",
    "    * Select all candidates with \"excellent\" and \"good\" predictions for the next round of interviews.\n",
    "\n",
    "\n",
    "#### A few technical notes:\n",
    "* If you're wondering what we mean by distance, please skim through this [article](https://towardsdatascience.com/importance-of-distance-metrics-in-machine-learning-modelling-e51395ffe60d).  In some cases distance measures are very intuitive.  For example if Jane has a 3.9 GPA, and a candidate Sally has a 3.8 GPA.  There is little distance between these two  with regard to that measure.  But other times distance measures can be far less intuitive.  For example, consider how we've encoded race.  It seems really wacky, that one race can be more or less distant from another race. Think back to where you considered potential worries regarding some of the ways these features were encoded.  This is where those concerns might come up! \n",
    "* A second difficulty that comes into play is that given the distance measure/function we choose, different features will contribute more or less distance to the total calculation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelEmployees = walmartDataset.loadModelEmployees(modelEmployees=default, featuresChosen=default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = walmartDataset.runNN()\n",
    "walmartDataset.makeSelections()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q3: Interrogating the Nearest Neighbor Model (1 hr)\n",
    "* Based on the resultant selections, detail any sources of liability you find based on your analyzing the model's performance. Please carefully distinguish and detail the claim and justification, providing visualizations of what you have found using the tools we have provided to you.\n",
    "    * In addition to analyzing the candidates who received positive \"excellent\" and \"good\" labels, consider who received \"poor\" labels.  What patterns do you notice?\n",
    "    \n",
    "* Suppose you found no 4/5ths violation with respect to those who were selected but it was clear that there was disparity in those who were not selected.  Would it be possible to formulate a Title VII claim under those circumstances?  If so how might you articulate the claim. Use evidence from the previous question. \n",
    "\n",
    "* Give the model employees and the algorithm presented - please explain how this bias is arising.  How are our model employees potentially contributing to the disparity?  How is the dataset contributing to the disparity?\n",
    "\n",
    "* Try mitigating the 4/5ths bias for the candidates selected by changing the race of the excellent model employee. Analyze performance again.  Can you spot any new potential 4/5ths violation due to your change?  Additionally is it lawful to arbitrarily perform \"data augmentation\" or manipulation like this in the context of hiring algorithms?  Justify your response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Logistic Regression Model (45 mins)\n",
    "Note to self: Hard to explain in just a block of text and also students don't need to know specifics.  Just want to use this as a vehicle for them to briefly learn about loss and optimization.  Present as something like - For an in-depth review of logistic regression and how this model functions please see this link. \n",
    "(We will construct a 15-min video or so for this)\n",
    "\n",
    "Note to self:  Because logistic regression is more complex our goal here would just be to give law\n",
    "    students a brief understanding of the foundations of neural networks and then just have them stick to more\n",
    "    theoretical concerns with regard to the law.\n",
    "\n",
    "Let's first train the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Students can visualize the loss and train test accuracy increasing.\n",
    "let model = walmartDataset.fitLogRegModel()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q4: Interrogating the Logistic Regression Model\n",
    "* Based on the resultant selections, detail any sources of liability you find based on your analyzing the model's performance. Please carefully distinguish and detail the claim and justification, providing visualizations of what you have found using the tools we have provided to you.\n",
    "* Detail three ways you could try and alter the model's performance so as to mitigate the source of liability you found. (Answers here would be - adding in new training samples.  manually adjusting weights, changing the training samples, etc.).\n",
    "* Compared to the ranking model - how might this model be superior for the task of selecting the best candidates? However, why still may it be a wise idea to stick with simpler models (think transparency)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Task 3: Alternative Sources of Data\n",
    "The HR Engineering Team, aware of your comprehensive investigation and company-critical project is worried that the initial dataset given to you may be flawed.  Fortunately, they came across voice recordings of the 500 candidates that were saved from their HIreVue online interviews.  \n",
    "\n",
    "While HireVue produces their own score available in the dataset, the team had begun to build a\n",
    "deep Neural Network to conduct audio analysis and try to score the candidates using their own in-house algorithm.  \n",
    "\n",
    "Here are a few samples of the audio recordings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_data = '/../../candidate1.wav'\n",
    "x , sr = librosa.load(audio_data)\n",
    "print(type(x), type(sr))#<class 'numpy.ndarray'> <class 'int'>print(x.shape, sr)#(94316,) 22050\n",
    "# Displays a play the audio button when run...\n",
    "ipd.Audio(audio_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q5:  Alternative Data Sources (15 mins)\n",
    "* Based on what you heard in the audio samples, are there any new legal risks to deploying a hiring algorithm that anlyzes voice?\n",
    "* Detail how under the ADA, a claim for employment discrimination could be brought against a candidate denied on the basis of results from a hiring algorithm analyzing voice.  \n",
    "* What features would you want such an algorithm analyzing audio to pick onto as an employer?  What features might the algorithm pick up on that are worrisome from the standpoint of legal liability?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great job!  Let's move onto Milestone 2 ... time to build your own compliant hiring algorithm!!! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
